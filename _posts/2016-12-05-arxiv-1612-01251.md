---
title: "Known Unknowns: Uncertainty Quality in Bayesian Neural Networks"
source: "http://arxiv.org/pdf/1612.01251v1"
authors:
  - "Ramon Oliveira"
  - "Pedro Tabacof"
  - "Eduardo Valle"
tags:
  - arxiv
published_in:
  - moonstruckcamel-research
abstract: |
  We evaluate the uncertainty quality in neural networks using anomaly
  detection. We extract uncertainty measures (e.g. entropy) from the predictions
  of candidate models, use those measures as features for an anomaly detector,
  and gauge how well the detector differentiates known from unknown classes. We
  assign higher uncertainty quality to candidate models that lead to better
  detectors. We also propose a novel method for sampling a variational
  approximation of a Bayesian neural network, called One-Sample Bayesian
  Approximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We
  compare the following candidate neural network models: Maximum Likelihood,
  Bayesian Dropout, OSBA, and --- for MNIST --- the standard variational
  approximation. We show that Bayesian Dropout and OSBA provide better
  uncertainty information than Maximum Likelihood, and are essentially equivalent
  to the standard variational approximation, but much faster.
  
---
