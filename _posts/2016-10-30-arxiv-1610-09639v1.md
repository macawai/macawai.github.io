---
title: "Compact Deep Convolutional Neural Networks With Coarse Pruning"
source: "http://arxiv.org/pdf/1610.09639v1"
authors:
  - "Sajid Anwar"
  - "Wonyong Sung"
tags:
  - arxiv
published_in:
  - psychodonkey-research-feature
abstract: |
  The learning capability of a neural network improves with increasing depth at
  higher computational costs. Wider layers with dense kernel connectivity
  patterns furhter increase this cost and may hinder real-time inference. We
  propose feature map and kernel level pruning for reducing the computational
  complexity of a deep convolutional neural network. Pruning feature maps reduces
  the width of a layer and hence does not need any sparse representation.
  Further, kernel pruning converts the dense connectivity pattern into a sparse
  one. Due to coarse nature, these pruning granularities can be exploited by GPUs
  and VLSI based implementations. We propose a simple and generic strategy to
  choose the least adversarial pruning masks for both granularities. The pruned
  networks are retrained which compensates the loss in accuracy. We obtain the
  best pruning ratios when we prune a network with both granularities.
  Experiments with the CIFAR-10 dataset show that more than 85% sparsity can be
  induced in the convolution layers with less than 1% increase in the
  missclassification rate of the baseline network.
snippet: |
  Neural networks can be big and slow. We often want them to run faster and use
  fewer resources. There has been much research on achieving this by
  trying to reduce the size of the NN, pruning it in a way that minimises the damage to the
  performance. For a CNN the bulk of the calculations and variables are found in
  the kernels, and it is reduction of these that this paper addresses.

  Typically pruning has been done in a variety of ways with one end of the specturm being
  pruning whole feature layers, and at the other end the zeroing of individual kernel
  entries. Removing whole feature layers is fairly brutal, while zeroing individual
  entries often does not lead to significant speedups or size reductions.

  This paper advocates an intermediate approach where individual kernel layers are
  removed. This seems to provide a good compromise, and performs well on some
  of the standard sets, with Induction of 85% sparsity resulting in less than 1% missclassification.

---
