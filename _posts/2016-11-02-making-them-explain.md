---
title: Making Neural Networks explain themselves for their decisions
source: http://sciencebulletin.org/archives/7082.html
published_in:
  - psychodonkey-news-feature
---

Neural Networks are fun, but weird right? _So_ weird that throwing random noise at them to reveal underlying structure makes us go "oooow look at all that weirdness" (remember [Deep Dream](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)). And despite their impressive results, one of the drawbacks to Neural Networks is that they are often the blackest of black boxes. Great results, no idea why. Not content with this, some MIT researchers have proposed a technique to reveal the underlying rationales for the decisions made by these systems, by mapping between decision outputs and a corpus of "short and coherent" pieces of text rationales "sufficient for making the same prediction". Giving us humans a few explanatory crumbs to feel comfortable with machine decisions will go a long way to assuaging that vague "yes but why" feeling that may linger, particularly in more sensitive areas like medicine or you know, [killer robots](http://macawai.com/weekly/2016-10-27-ethical-determinism-trolley-terminator-unemployment). For as the authors say in their opening "Prediction without justification has limited applicability". Check out the full MIT research paper ["Rationalizing Neural Predictions"](https://people.csail.mit.edu/taolei/papers/emnlp16_rationale.pdf).
