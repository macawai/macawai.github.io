---
title: "Neural Multigrid"
source: "http://arxiv.org/pdf/1611.07661v1"
authors:
  - "Tsung-Wei Ke"
  - "Michael Maire"
  - "Stella X. Yu"
tags:
  - arxiv
published_in:
  - unhingedoryx-research
abstract: |
  We propose a multigrid extension of convolutional neural networks (CNNs).
  Rather than manipulating representations living on a single spatial grid, our
  network layers operate across scale space, on a pyramid of tensors. They
  consume multigrid inputs and produce multigrid outputs; convolutional filters
  themselves have both within-scale and cross-scale extent. This aspect is
  distinct from simple multiscale designs, which only process the input at
  different scales. Viewed in terms of information flow, a multigrid network
  passes messages across a spatial pyramid. As a consequence, receptive field
  size grows exponentially with depth, facilitating rapid integration of context.
  Most critically, multigrid structure enables networks to learn internal
  attention and dynamic routing mechanisms, and use them to accomplish tasks on
  which modern CNNs fail.
  Experiments demonstrate wide-ranging performance advantages of multigrid. On
  CIFAR image classification, flipping from single to multigrid within standard
  CNN architectures improves accuracy at modest compute and parameter increase.
  Multigrid is independent of other architectural choices; we show synergistic
  results in combination with residual connections. On tasks demanding per-pixel
  output, gains can be substantial. We show dramatic improvement on a synthetic
  semantic segmentation dataset. Strikingly, we show that relatively shallow
  multigrid networks can learn to directly perform spatial transformation tasks,
  where, in contrast, current CNNs fail. Together, our results suggest that
  continuous evolution of features on a multigrid pyramid could replace virtually
  all existing CNN designs.
  
---
