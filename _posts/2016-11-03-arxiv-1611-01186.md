---
title: "Demystifying ResNet"
source: "http://arxiv.org/pdf/1611.01186v1"
authors:
  - "Sihan Li"
  - "Jiantao Jiao"
  - "Yanjun Han"
  - "Tsachy Weissman"
tags:
  - arxiv
  - needs-commentary
published_in:
  - grimsheep-research
abstract: |
  We provide a theoretical explanation for the superb performance of ResNet via
  the study of deep linear networks and some nonlinear variants. We show that
  with or without nonlinearities, by adding shortcuts that have depth two, the
  condition number of the Hessian of the loss function at the zero initial point
  is depth-invariant, which makes training very deep models no more difficult
  than shallow ones. Shortcuts of higher depth result in an extremely flat
  (high-order) stationary point initially, from which the optimization algorithm
  is hard to escape. The 1-shortcut, however, is essentially equivalent to no
  shortcuts. Extensive experiments are provided accompanying our theoretical
  results. We show that initializing the network to small weights with
  2-shortcuts achieves significantly better results than random Gaussian (Xavier)
  initialization, orthogonal initialization, and shortcuts of deeper depth, from
  various perspectives ranging from final loss, learning dynamics and stability,
  to the behavior of the Hessian along the learning process.
  
---
