---
title: "Lie-Access Neural Turing Machines"
source: "http://arxiv.org/pdf/1611.02854v1"
authors:
  - "Greg Yang"
  - "Alexander M. Rush"
tags:
  - arxiv
  - needs-commentary
published_in:
  - screamingsloth-research-feature
abstract: |
  Recent work has demonstrated the effectiveness of employing explicit external
  memory structures in conjunction with deep neural models for algorithmic
  learning (Graves et al. 2014; Weston et al. 2014). These models utilize
  differentiable versions of traditional discrete memory-access structures
  (random access, stacks, tapes) to provide the variable-length storage necessary
  for computational tasks. In this work, we propose an alternative model,
  Lie-access memory, that is explicitly designed for the neural setting. In this
  paradigm, memory is accessed using a continuous head in a key-space manifold.
  The head is moved via Lie group actions, such as shifts or rotations, generated
  by a controller, and soft memory access is performed by considering the
  distance to keys associated with each memory. We argue that Lie groups provide
  a natural generalization of discrete memory structures, such as Turing
  machines, as they provide inverse and identity operators while maintain
  differentiability. To experiment with this approach, we implement several
  simplified Lie-access neural Turing machine (LANTM) with different Lie groups.
  We find that this approach is able to perform well on a range of algorithmic
  tasks.
snippet: |
  Improvements in differentiable memory techniques have produced some significant
  advances in the ability of RNNs to solve complicated problems. The
  [google paper](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz) in nature a few weeks ago was a good example of this.

  This paper approaches generalising existing memory models from a mathematical
  viewpoint. Separate read and
  write heads are associated with elements of an underlying Lie Group (the key group).
  Writes store the current write key and a value value vector as a "memory"
  in a list. Reads sum all the stored memories weighted by "distance" of the read key
  to each memories keys. At each iteration of the RNN the read and write heads are
  updated by applying an update based on the current state.

  The compares favourably with existing methods in its ability to learn simplified
  tasks.



---
