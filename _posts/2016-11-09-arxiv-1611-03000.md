---
title: "Bio-Inspired Spiking Convolutional Neural Network using Layer-wise   Sparse Coding and STDP Learning"
source: "http://arxiv.org/pdf/1611.03000v1"
authors:
  - "Amirhossein Tavanaei"
  - "Anthony S. Maida"
tags:
  - arxiv
  - needs-commentary
published_in:
  - screamingsloth-research
abstract: |
  Hierarchical feature discovery using non-spiking convolutional neural
  networks (CNNs) has attracted much recent interest in machine learning and
  computer vision. However, it is still not well understood how to create spiking
  deep networks with multi-layer, unsupervised learning. One advantage of spiking
  CNNs is their bio-realism. Another advantage is that they represent information
  using sparse spike-trains which enable power-efficient implementation. This
  paper explores a novel bio-inspired spiking CNN that is trained in a greedy,
  layer-wise fashion. The proposed network consists of a spiking
  convolutional-pooling layer followed by a feature discovery layer. Kernels for
  the convolutional layer are trained using local learning. The learning is
  implemented using a sparse, spiking auto-encoder representing primary visual
  features. The feature discovery layer is equipped with a probabilistic
  spike-timing-dependent plasticity (STDP) learning rule. This layer represents
  complex visual features using probabilistic leaky, integrate-and-fire (LIF)
  neurons. Our results show that the convolutional layer is stack-admissible,
  enabling it to support a multi-layer learning. The visual features obtained
  from the proposed probabilistic LIF neurons in the feature discovery layer are
  utilized for training a classifier. Classification results contribute to the
  independent and informative visual features extracted in a hierarchy of
  convolutional and feature discovery layers. The proposed model is evaluated on
  the MNIST digit dataset using clean and noisy images. The recognition
  performance for clean images is above 98%. The performance loss for recognizing
  the noisy images is in the range 0.1% to 8.5% depending on noise types and
  densities. This level of performance loss indicates that the network is robust
  to additive noise.
  
---
