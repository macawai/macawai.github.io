---
title: "Optimizing Memory Efficiency for Deep Convolutional Neural Networks on   GPUs"
source: "http://arxiv.org/pdf/1610.03618v1"
authors:
  - "Chao Li" # Department of Electrical and Computer Engineering, North Carolina State University
  - "Yi Yang" #  Department of Integrated System, NEC Labs America
  - "Min Feng" #  Department of Integrated System, NEC Labs America
  - "Srimat Chakradhar" #  Department of Integrated System, NEC Labs America
  - "Huiyang Zhou" #  Department of Electrical and Computer Engineering, North Carolina State University
tags:
  - arxiv
  - CNN
  - performance
published_in:
  - crazydragon-research-feature
---

# Summary

Memory locality, data layouts and memory access patterns can have a deceptively
large effect on the performance of implementation of seemingly simple
algorithms - especially in heavily multithreaded or distributed systems.  This
paper shows that slow-downs due to this kind of effect are present in modern
CNN libraries, and their extent varies depending upon not only the library, but
the CNN topology and layer types. However existing libraries only use one data layout
for all their CNN layers - which means there is potential for significant
performance losses in.


This paper shows that choosing the right memory layout for each layer, to
optimize the data access patterns, can result in savings of upto 30x for a layer
or 6x for the whole network.

## Abstract

>   Leveraging large data sets, deep Convolutional Neural Networks (CNNs) achieve
> state-of-the-art recognition accuracy. Due to the substantial compute and
> memory operations, however, they require significant execution time. The
> massive parallel computing capability of GPUs make them as one of the ideal
> platforms to accelerate CNNs and a number of GPU-based CNN libraries have been
> developed. While existing works mainly focus on the computational efficiency of
> CNNs, the memory efficiency of CNNs have been largely overlooked. Yet CNNs have
> intricate data structures and their memory behavior can have significant impact
> on the performance. In this work, we study the memory efficiency of various CNN
> layers and reveal the performance implication from both data layouts and memory
> access patterns. Experiments show the universal effect of our proposed
> optimizations on both single layers and various networks, with up to 27.9x for
> a single layer and up to 5.6x on the whole networks.
>
