---
title: "Combating Reinforcement Learning's Sisyphean Curse with Intrinsic Fear"
source: "http://arxiv.org/pdf/1611.01211v2"
authors:
  - "Zachary C. Lipton"
  - "Jianfeng Gao"
  - "Lihong Li"
  - "Jianshu Chen"
  - "Li Deng"
tags:
  - arxiv
  - needs-commentary
published_in:
  - grimsheep-research
abstract: |
  To use deep reinforcement learning in the wild, we might hope for an agent
  that would never make catastrophic mistakes. At the very least, we could hope
  that an agent would eventually learn to avoid old mistakes. Unfortunately, even
  in simple environments, modern deep reinforcement learning techniques are
  doomed by a Sisyphean curse. Owing to the use of function approximation, these
  agents eventually forget experiences as they become exceedingly unlikely under
  a new policy. Consequently, for as long as they continue to train,
  state-aggregating agents may periodically relive catastrophic mistakes. We
  demonstrate unacceptable performance of deep Q-networks on two toy problems. We
  then introduce intrinsic fear, a method that mitigates these problems by
  avoiding dangerous states.
  
---
