---
title: "Capacity and Trainability in Recurrent Neural Networks"
source: "http://arxiv.org/pdf/1611.09913v1"
authors:
  - "Jasmine Collins"
  - "Jascha Sohl-Dickstein"
  - "David Sussillo"
tags:
  - arxiv
published_in:
  - moonstruckcamel-research
abstract: |
  Two potential bottlenecks on the expressiveness of recurrent neural networks
  (RNNs) are their ability to store information about the task in their
  parameters, and to store information about the input history in their units. We
  show experimentally that all common RNN architectures achieve nearly the same
  per-task and per-unit capacity bounds with careful training, for a variety of
  tasks and stacking depths. They can store an amount of task information which
  is linear in the number of parameters, and is approximately 5 bits per
  parameter. They can additionally store approximately one real number from their
  input history per hidden unit. We further find that for several tasks it is the
  per-task parameter capacity bound that determines performance. These results
  suggest that many previous results comparing RNN architectures are driven
  primarily by differences in training effectiveness, rather than differences in
  capacity. Supporting this observation, we compare training difficulty for
  several architectures, and show that vanilla RNNs are far more difficult to
  train, yet have higher capacity. Finally, we propose two novel RNN
  architectures, one of which is easier to train than the LSTM or GRU.
  
---
